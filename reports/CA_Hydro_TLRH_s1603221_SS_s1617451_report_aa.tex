%                                                                 aa.dem
% AA vers. 8.2, LaTeX class for Astronomy & Astrophysics
% demonstration file
%                                                       (c) EDP Sciences
%-----------------------------------------------------------------------
%
%\documentclass[referee]{aa} % for a referee version
%\documentclass[onecolumn]{aa} % for a paper on 1 column
%\documentclass[longauth]{aa} % for the long lists of affiliations
%\documentclass[rnote]{aa} % for the research notes
%\documentclass[letter]{aa} % for the letters
%\documentclass[bibyear]{aa} % if the references are not structured
% according to the author-year natbib style

%
\documentclass{aa}

%
\usepackage{graphicx}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{txfonts}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hyperref}
\hypersetup{pdfborder=0 0 0, colorlinks=true, linkcolor=black, urlcolor=blue,
citecolor=black}
% To add links in your PDF file, use the package "hyperref"
% with options according to your LaTeX or PDFLaTeX drivers.
%
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\newcommand{\Sun}[0]{\ensuremath{_{\odot}}}
\newcommand{\mytilde}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}
\begin{document}


   \title{Star Cluster and Giant Molecular Cloud Interaction with AMUSE}

   \subtitle{Computational Astrophysics (CA) assignment three}

   \author{T. Halbesma (1603221)
          \inst{1}
          \and
          S. Sultan (1617451)\inst{2}
          }

   \institute{Anton Pannekoek Instituut (API), University of Amsterdam,
              Science Park 904, 1098 XH Amsterdam
              \email{timo.halbesma@student.uva.nl}
         \and
             Informatics Institute, Section Computational Science, University of Amsterdam,
             Science Park 904, 1098 XH Amsterdam
             \email{shabaz.sultan@student.uva.nl}
             }

   %\date{Received September 15, 1996; accepted March 16, 1997}

% \abstract{}{}{}{}{}
% 5 {} token are mandatory

  \abstract
  % context heading (optional)
  % {} leave it empty if necessary
   {}
  % aims heading (mandatory)
   {}
  % methods heading (mandatory)
   {}
  % results heading (mandatory)
   {}
  % conclusions heading (optional), leave it empty if necessary
   {}

   \keywords{Hydrodynamics --
                AMUSE --
               }

   \maketitle
%
%________________________________________________________________

\section{Introduction}
In this report, the authors have used the Astrophysical Multipurpose Software Environment, in short AMUSE \citep{2009NewA...14..369P, 2013CoPhC.183..456P, 2013A&A...557A..84P}. Firstly, a stellar cluster is created where the gravitational interactions between the stars are calculated by the N-Body algorithm developed by Barnes and Hut, the BHTree \cite{1986Natur.324..446B}. In addition, we set up a giant molecular cloud (GMC) using Smoothed Particle Hydrodynamics (SPH) \citep{1977MNRAS.181..375G} to simulate the gas and dust in the cloud. In particular, we use the TreeSPH hydrodynamics code `FI' \citep{1989ApJS...70..419H, 1997A&A...325..972G, 2004A&A...422...55P, 2005PhDT........17P}. Choosing an SPH hydrodynamics code enables us to study the gravity in the system as if two N-Body systems are interacting because the gas is split up in different regions represented by SPH particles. The gravity code can act on these particles. In AMUSE two (or more) gravity systems can be coupled using the Bridge interface. The Bridge interface, thus, is used to gravitationally couple the GMC and the stellar cluster.

% \begin{table*}
%     \caption{General properties of the Stellar Evolution codes \citep{AMUSEdocumentation}}
%     \label{tab:algorithms}
%     \centering
%     \begin{tabular}{l l l l l l l l }
%         \hline\hline
%         Algorithm & Type & Binary/Single & Metallicity $z$ & Mass $m$ (M\Sun) \\
%         \hline
%         MESA & Henyey & Both & All, even 0 & 0.1 - 100 \\
%         EVTwin & Henyey & Single\footnotemark[1] & 0.02 only & ? \\
%         SSE & Parameterized & Single & 0.0001 - 0.03 & 0.1 - 100 \\
%         SeBa & Parameterized & Both & 0.0001 - 0.03 & 0.1 - 100 \\
%         \hline
%     \end{tabular}
% \end{table*}
% \addtocounter{footnote}{1}
% \footnotetext[\value{footnote}]{Binaries are not yet available in AMUSE EVTwin Interface.}

% \setlength\tabcolsep{2pt}
% \begin{table}
%     \caption[]{Parameters and results of simulations. Here, $z$ is the metallicity, $t_{\rm end}$ the end time of the simulation, $L$ the luminosity, $M$ the mass, $R$ the radius and Runtime is the wall-clock runtime of the algorithm.}
%     \label{tab:parameterspace}
%     \begin{tabular}{lcccccc}
%         \hline
%         \noalign{\smallskip}
%         Algorithm & $z$ & $t_{\rm end} (Myr)$ & $L$ (L\Sun) & $M$ (M\Sun) & $R$ (R\Sun) & Runtime (s) \\
%         \hline
%         \noalign{\smallskip}
%         MESA & 0.02 & 4600 & 1.034 & 1 & 1.016 & 9.54 \\
%         EVTwin  & 0.02 & 4600 & 1.027 & 1 & 0.1834 & 9.58 \\
%         SSE & 0.02 & 4600 & 0.9585 & 1 & 0.9856 & 0.166 \\
%         SeBa & 0.02 & 4600 & 0.9585 & 1 & 0.9856 & 0.554 \\
%         MESA & 0.02 & 10 & 7460 & 10 & 5.091 & 10.9 \\
%         EVTwin & 0.02 & 10 & 7637 & 10 & 0.8764 & 4.67  \\
%         SSE & 0.02 & 10 & 7233 & 10 & 4.930 & 0.0984 \\
%         SeBa & 0.02 & 10 & 7285 & 10 & 4.938 & 0.133 \\
%         \hline
%         \noalign{\smallskip}
%         \noalign{\smallskip}
%     \end{tabular}
% \end{table}

\section{Answers to questions}

% Not sure is this has been paraphrased correctly below.
% .... from AMUSE documentation: ``The bridgesys is flexible but care should be taken in order to obtain valid results. For one thing, there is no restriction or check on the validity of the assumption of well seperated dynamics: for example any system could be split up and put together in bridge, but if the timestep is chosen to be larger than the timestep criterion of the code, the integration will show errors.'' ....

% This question can be answered right after the part in the introduction where we mention the Bridge interface.
\textit{The code contains both an N-body code (gravity) and an SPH code (hydro) but no call to evolve model for either code. Where is evolve model called for these codes, and with which time interval is it called?} The Bridge interface has a built-in function bridge.evolve(), that will evolve all gravitational systems added to the bridge. The time step used to evolve the GMC and the cluster is given upon initialization of the Bridge. Here, we use 0.1 Myr as time interval. Note that one ought to be cautions with the choice of time interval. If the chosen time interval is smaller than the timestep criterion of the integrator attached to the Bridge, the integrator algorithm breaks down.

% .... from AMUSE documentation: ``The main idea is that systems experience each others gravity through periodic velocty kicks with ordinary evolution in between - the evolution is thus described by an alternation of drift (D) and kick (K) operators, here chosen as:

%   K(1/2 dt) D(dt) K(1/2 dt)'' ....

% This question can also be answered right after the part in the introduction where we mention the Bridge interface.
\textit{How is the gravitational force of the gas on the cluster calculated? And of the cluster on the gas? What errors could this introduce? How could these be solved?}
By adding systems to the Bridge interface, the systems themselves are gravitationally evolved with the most appropriate algorithm. The system as a whole is evolved in a self-consistent manner. In this particular implementation, the gravitational force of the gas on the cluster is calculated by the Bridge interface. This is true as well for the gravity of the cluster acting on the gas in the GMC (the SPH particles). The way this is done is, firstly, by calculating the `kick' one system experiences due to the gravity of the other system over time $dt/2$. Secondly, the evolution of both systems separately changes the cluster and GMC properties over time $dt$. Thirdly, another `kick' event takes place over time $dt/2$. Finally, these three steps are repeated until the end time of the simulation has been reached.

% .... from AMUSE documentation: ``There is an issue with the synchronization: some codes do not end on the exact time of an evolve, or need an explicit sync call. In these cases it is up to the user to determine whether bridge can be used (an explicit sync call may induce extra errors that degrade the order of the integrator)'' ....
This might prove problematic when REPLACE. The errors introduced are REPLACE.

\textit{What is the mass of a 'star' in the star cluster. What is the mass of a 'gas' particle in the GMC. Is this a problem and why?} The stars are represented by the particles parameter, that is the result of a function call to the new\_plummer\_model function. This is an implementation of the Plummer model \citep{1911MNRAS..71..460P}. This means that all stars have the same mass that is given by the division of the total cluster mass by the number of stars. By default, this is $10^6$ MSun/128 stars gives a mass of 7812.5 Msun per star. Note that this may change due to choosing different cluster masses and/or different numbers of stars. For the gas all masses are equal too for each SPH particle. Given the default total mass of $10^7$ MSun 1024 divided over 1024 SPH particles, the mass is \~9766 MSun per gas SPH particle in the GMC. 

The per-particle mass of the SPH particles is slightly higher than the mas of a star. We do not consider this a problem. However, the stellar mass is three decades higher than unity. This means that the stellar evolution would occur on timescales of roughly a year. For this simulation stellar evolution is neglected, but such high mass stars are not physical and is therefore a problem. In addition, a problematic assumption of the the Plummer sphere is that all stars have the same mass. Initializing the stars with a more realistic initial mass function would yield more realistic, more physical results. To use the Plummer model as a mere toy model is interesting though, albeit with the aforementioned caveats.

\textit{What is the smoothing length of the SPH kernel function? What does that mean?}
The smoothening length of the SPH kernel function is the spatial distance between the particles representing an area of gas.
% Fi.py line 1899: ``Eps-is-h-flag. True means: set gas particles gravitational epsilon is set to h (SPH smoothning length)''. In CA_HD.py line 57 we set the eps_is_h_flag to True.
% Pelupessy 2005 (PhD thesis) implies that for W(r; h) we take the spline kernel of Monaghan & Lattanzio (1985). This could provide insight into what the smoothening length is?

\textit{What is the u parameter on the gas particles? How does it affect the simulation? Explain what happens with the orbital parameters calculated in put in orbit if you set $v_\infty = 0$ and why?}

% .... Python documentation: ``To change a sequence you are iterating over while inside the loop (for example to duplicate certain items), it is recommended that you first make a copy. Looping over a sequence does not implicitly make a copy. The slice notation makes this especially convenient:'' ....
% .... AMUSE documentation: `` Part of interaction between codes in AMUSE is based on exchanging data between the community codes or exchanging data between these codes and AMUSE. As you might have noticed in the pervious tutorial topic, every code provides access to particle collections or grids. The data of these collections or grids live inside the code, while the data of collections created in the script live inside the python process.

% Background: All data storage of particle collections (or grids) is implemented by different storage classes. AMUSE supports storage classes that simply store the data in python lists and numpy arrays. AMUSE also supports storage classes that send messages to the codes to perform the actual storage and retrieval. At the script level the interface to these classes is all the same, so in normal use they behave the same. The performance of the different storage classes will vary, for a code storage the data may be sent over an internet connection causing slower reaction times. Smart usage of channels and caching data in memory sets will increase performance. '' ....
% NB this might be utterly untrue, but what else could the copy do?
\textit{Explain in detail what the call channels.copy() does.}
The Channels class is an object that holds an instance of a list named \_channels as a class variable. An item is appended to the channels list when the add\_channel function is called (here, once for the GMC and once for the stellar cluster). The Python documentation states that it is good practice to copy an instance of a list prior to iteration. As for the AMUSE implementation of the Channel object, the copy function loops over all channels added to the self.\_channels list and makes a copy of it. According to the AMUSE documentation this is done because the parameters in the channel list require updating them after running the evolve method. The channel list `lives' in the Python script whereas the evolved parameters `live' in the underlying community code. The copy function call takes care of the data exchange between several codes.

% Placement could be as part of the comments on Fig.~\ref{fig:GMC_with_cluster}
% todo: check the sizes of the particles.
\textit{Why did we choose to make the SPH particles look bigger and partially transparent?} Fig.~\ref{fig:GMC_with_cluster} shows the GMC and cluster. Notice that the SPH particles are bigger and partially transparent because, in fact, they are. The SPH particles are a representation of an entire region of gas. The stars should be able to move trough the SPH only interacting with the region of gas in part. As indicated above, the SPH particles are heavier than the stars. The size might be used as an indication of mass rather than of physical size. REPLACE (idk if this is true) In addition, the smoothening length of the kernel is much bigger than the radius of the star. The physical size of the SPH particles are, in fact, bigger than the stars. It stands to reason that the SPH particles are therefore bigger plotted but also that they are transparent.

\textit{Copy and rewrite the movie code to only produce a single frame of this movie (as pdf).} See Fig.~\ref{fig:GMC_with_cluster}.

\begin{figure}
    \centering
    \includegraphics[width=\hsize]{img/test-44-8.png}
    \caption{Snapshot of the GMC (blue, centered) and the Cluster (black) orbiting around the cluster.}\label{fig:GMC_with_cluster}
\end{figure}

\textit{Vary the initial parameters of the simulation and produce an image of the following outcomes, if one is not possible, discuss why:}
\begin{itemize}
    \item Both star cluster and GMC remain intact.
    \item The star cluster is disrupted by the GMC.
    \item The GMC is disrupted by the star cluster.
    \item The star cluster takes some gas from the GMC.
    \item The star cluster is captured and remains inside the GMC.
\end{itemize}

\section{Multiple GMCs}
When a star cluster moves through its neighbourhood it may interact with not just one, but multiple gas clouds. To effectively simulate the cluster interacting with multiple distinct GMCs they need to be spaced far enough apart to not interact with eachother. I.e. we do not want the GMCs to merge, otherwise the simulation is simulating a scenario where the cluster is still interacting with just one GMC, that just happens to be bigger. Ideally the cluster is really only interacting with one GMC in the simulation at a time. Adding the extra GMCs does add computational complexity. First of all the simulated time needs to be longer, because the time that the cluster is traveling in between the GMCs needs to be added; the simulation is not just simulating the time the cluster is actually interacting with a GMC anymore. This is dependent on the spacing in between GMCs, which is ideally large so that there is not too much interaction between GMCs. Making this distance too large however can blow up the simulation time.

The second computational challenge is that even if the cluster is only interacting with one GMC at a time, adding the extra GMCs will have the hydrodynamics of the currently non-interacting GMCs be simulated as well during the entire simulation run. This adds overhead. Because adding extra GMCs to the simulation increases the number of years that need to be simulated, timing a full simulation run with different number of GMCs does not give full insight into this extra overhead. It may just be the extra simulation time added by the travel time between GMCs that is measured. Instead a fixed 4Myr simulation run is timed for simulations with one to ten GMCs. This is showcased in figure~\ref{fig:multi_GMCs_timings}, where it can be seen that the computation time for a set amount of simulated years increases linearly as more GMCs are added. This could be considered undesirable, because we're only interested in GMCs when they're interacting with the cluster. When they're not, their simulation could be wasted computational resources (given the important assumption that no significant change from the initial states of the GMCs occurs until they start interacting with the cluster).

\begin{figure}
    \centering
    \includegraphics[width=\hsize]{img/multi_gmc_timings.png}
    \caption{Number of seconds that 4Myr simulation needs to run, showcasing a linear increase in the computation time as more GMCs are added to the simulation.}\label{fig:multi_GMCs_timings}
\end{figure}
%\textit{How does the computation time scale with the number of GMC's, assuming that each GMC has the same initial mass and number of particles.}

%\textit{The impact parameters ($d$ and $v_\infty$) between the star cluster and the first GMC are easy to choose. Discuss different methods to choose the impact parameters between the star cluster and the later clusters, what is the main challenge here?}

%\textit{Run simulations to find out when and how the second encounter is qualitatively different from a single encounter. Use the different scenarios from assignment 3C as a starting point.}

%\textit{Which changes in the star cluster's structural parameters can explain these differences.}



\section{Discussion}\label{sec:discussion}

\section{Conclusions}\label{sec:conclusions}
\begin{enumerate}
    \item
    \item
    \item
    \item
\end{enumerate}


\begin{acknowledgements}
The authors are grateful for the help of their supervisors Edwin van der Helm, MSc and Prof. dr. S.F. Portegies Zwart. \\

This research has made use of NASA's Astrophysics Data System.
\end{acknowledgements}


%-------------------------------------------------------------------

\bibliographystyle{aa}
%\setlength{\bibsep}{0pt} % Remove whitespace in bibliography.
\bibliography{CA_Hydro_TLRH_s1603221_SS_s1617451_report_aa}
\end{document}
